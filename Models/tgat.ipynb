{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0329cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Training A3TGCN2 on 2017 data (grouped by fixed event count)...\n",
      "Epoch 1 Loss: 799.0663\n",
      "Epoch 2 Loss: 1466.5138\n",
      "Epoch 3 Loss: 593.8443\n",
      "Epoch 4 Loss: 1053.6132\n",
      "Epoch 5 Loss: 744.2519\n",
      "\n",
      "üß™ Evaluating A3TGCN2 on 2018 test set (grouped by fixed event count)...\n",
      "\n",
      "=== Final Test Set Evaluation ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5689    0.4963    0.5302   1343256\n",
      "           1     0.0941    0.1221    0.1063    575394\n",
      "\n",
      "    accuracy                         0.3841   1918650\n",
      "   macro avg     0.3315    0.3092    0.3182   1918650\n",
      "weighted avg     0.4265    0.3841    0.4030   1918650\n",
      "\n",
      "Confusion Matrix:\n",
      "[[666707 676549]\n",
      " [505137  70257]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import entropy\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric_temporal.signal import DynamicGraphTemporalSignal\n",
    "from torch_geometric_temporal.nn.recurrent import A3TGCN2\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------------\n",
    "# 0. Set device\n",
    "# ---------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load & Preprocess Training Data\n",
    "# ---------------------------\n",
    "df_train = pd.read_csv('/content/drive/MyDrive/dsa4263/2017_data.csv')\n",
    "df_train['Timestamp'] = pd.to_datetime(df_train['Timestamp'], errors='coerce')\n",
    "df_train = df_train.dropna(subset=['Timestamp'])\n",
    "\n",
    "# Map label: benign flows become 0; everything else becomes 1\n",
    "df_train['Label'] = df_train['Label'].map({'BENIGN': 0, 'Benign': 0}).fillna(1).astype(int)\n",
    "\n",
    "# Define feature columns (exclude graph‚Äêrelated columns)\n",
    "graph_cols = ['Source IP', 'Destination IP', 'Label']\n",
    "feature_cols = [col for col in df_train.columns if col not in graph_cols + ['Flow ID', 'Timestamp']]\n",
    "\n",
    "# Clean numeric features\n",
    "df_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "scaler = StandardScaler()\n",
    "df_train[feature_cols] = scaler.fit_transform(imputer.fit_transform(df_train[feature_cols]))\n",
    "df_train[feature_cols] = df_train[feature_cols].astype(np.float64)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Enriched Node Feature Engineering\n",
    "# ---------------------------\n",
    "def build_node_features(sub_df):\n",
    "    \"\"\"\n",
    "    For a given sub-dataframe (e.g. a window of events),\n",
    "    aggregate the edge features per node and compute:\n",
    "      - Number of times a node appears as source,\n",
    "      - Number of times as destination,\n",
    "      - Entropy over destination IPs for that node.\n",
    "    The final node feature vector has dimension: len(feature_cols) + 3.\n",
    "    \"\"\"\n",
    "    src_counts = sub_df['Source IP'].value_counts().to_dict()\n",
    "    dst_counts = sub_df['Destination IP'].value_counts().to_dict()\n",
    "    ip_dst_entropy = defaultdict(list)\n",
    "    ip_feat_sum = defaultdict(lambda: np.zeros(len(feature_cols)))\n",
    "\n",
    "    # Accumulate feature vectors per source IP and record destination occurrences.\n",
    "    for _, row in sub_df.iterrows():\n",
    "        src, dst = row['Source IP'], row['Destination IP']\n",
    "        ip_feat_sum[src] += row[feature_cols].values.astype(np.float64)\n",
    "        ip_dst_entropy[src].append(dst)\n",
    "\n",
    "    # Get all IPs present (from sources or destinations)\n",
    "    all_ips = set(sub_df['Source IP']).union(set(sub_df['Destination IP']))\n",
    "    ip_to_idx = {ip: i for i, ip in enumerate(all_ips)}\n",
    "\n",
    "    # Build the node features: first the aggregated feature vector,\n",
    "    # then append the counts (as source & destination) and entropy.\n",
    "    node_features = np.zeros((len(ip_to_idx), len(feature_cols) + 3))\n",
    "    for ip, idx in ip_to_idx.items():\n",
    "        feat = ip_feat_sum[ip]\n",
    "        count_src = src_counts.get(ip, 0)\n",
    "        count_dst = dst_counts.get(ip, 0)\n",
    "        ent = entropy(pd.Series(ip_dst_entropy[ip]).value_counts(normalize=True)) if ip_dst_entropy[ip] else 0\n",
    "        node_features[idx] = np.concatenate([feat, [count_src, count_dst, ent]])\n",
    "    return torch.tensor(node_features, dtype=torch.float32), ip_to_idx\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build a Dynamic Graph (Grouping by a Fixed Number of Events)\n",
    "# ---------------------------\n",
    "def preprocess_dynamic_by_count(df, feature_cols, window_size=50):\n",
    "    \"\"\"\n",
    "    Create a dynamic graph signal from the DataFrame without minute binning.\n",
    "    The data is divided into windows of consecutive events (default = 50 events per window).\n",
    "    Each window is used to compute enriched node features, edge indices, edge attributes,\n",
    "    and targets.\n",
    "    \"\"\"\n",
    "    # Sort DataFrame by timestamp to preserve temporal order\n",
    "    df = df.sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "\n",
    "    num_windows = len(df) // window_size\n",
    "    edge_indices = []\n",
    "    edge_attrs = []\n",
    "    targets = []\n",
    "    features = []\n",
    "\n",
    "    for i in range(num_windows):\n",
    "        sub_df = df.iloc[i * window_size: (i + 1) * window_size]\n",
    "        node_feats, ip_to_idx = build_node_features(sub_df)\n",
    "\n",
    "        # Build edge index for the current window from source/destination IPs\n",
    "        try:\n",
    "            edge_index = torch.tensor([\n",
    "                [ip_to_idx[s] for s in sub_df['Source IP']],\n",
    "                [ip_to_idx[d] for d in sub_df['Destination IP']]\n",
    "            ], dtype=torch.long)\n",
    "        except KeyError:\n",
    "            # Skip this window if a mapping issue occurs\n",
    "            continue\n",
    "\n",
    "        # Edge attributes are the original (scaled) features\n",
    "        edge_attr = torch.tensor(sub_df[feature_cols].values.astype(np.float64), dtype=torch.float32)\n",
    "        target = torch.tensor(sub_df['Label'].values, dtype=torch.long)\n",
    "\n",
    "        edge_indices.append(edge_index)\n",
    "        edge_attrs.append(edge_attr)\n",
    "        targets.append(target)\n",
    "        # The node features are expected in shape: (batch_size, num_nodes, in_channels, 1)\n",
    "        features.append(node_feats.unsqueeze(0).unsqueeze(-1))\n",
    "\n",
    "    return DynamicGraphTemporalSignal(edge_indices, edge_attrs, features, targets)\n",
    "\n",
    "# Build the dynamic graph for training using fixed event grouping\n",
    "train_dataset = preprocess_dynamic_by_count(df_train, feature_cols, window_size=50)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Define the A3TGCN2-based Edge Classifier\n",
    "# ---------------------------\n",
    "class A3TGCN2_EdgeClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=2, batch_size=1):\n",
    "        \"\"\"\n",
    "        in_channels should match the dimension of the enriched node features (len(feature_cols) + 3).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.recurrent = A3TGCN2(in_channels=in_channels, out_channels=64, periods=1, batch_size=batch_size)\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight, edge_src, edge_dst):\n",
    "        x = x.to(device)\n",
    "        edge_index = edge_index.to(device)\n",
    "        edge_src = edge_src.to(device)\n",
    "        edge_dst = edge_dst.to(device)\n",
    "        if edge_weight is not None:\n",
    "            edge_weight = edge_weight.to(device)\n",
    "        h = self.recurrent(x, edge_index, edge_weight)[0]\n",
    "        h_src = h[edge_src]\n",
    "        h_dst = h[edge_dst]\n",
    "        edge_input = torch.cat([h_src, h_dst], dim=1)\n",
    "        return self.edge_mlp(edge_input)\n",
    "\n",
    "# The enriched node features have dimension: len(feature_cols) + 3\n",
    "in_channels = len(feature_cols) + 3\n",
    "model = A3TGCN2_EdgeClassifier(in_channels=in_channels).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Training Loop\n",
    "# ---------------------------\n",
    "print(\"‚è≥ Training A3TGCN2 on 2017 data (grouped by fixed event count)...\")\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    # Iterate over each snapshot (each window of events)\n",
    "    for t in range(len(train_dataset.features)):\n",
    "        edge_index = train_dataset.edge_indices[t]\n",
    "        edge_src = edge_index[0]\n",
    "        edge_dst = edge_index[1]\n",
    "        y_hat = model(train_dataset.features[t], edge_index, None, edge_src, edge_dst)\n",
    "        loss = loss_fn(y_hat, train_dataset.targets[t].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Preprocess & Evaluate on Test Set (2018)\n",
    "# ---------------------------\n",
    "df_test = pd.read_csv('/content/drive/MyDrive/dsa4263/ddos2018_cleaned.csv').iloc[:, :-2]\n",
    "df_test['Timestamp'] = pd.to_datetime(df_test['Timestamp'], errors='coerce')\n",
    "df_test = df_test.dropna(subset=['Timestamp'])\n",
    "\n",
    "# Map label similarly for test (e.g. 'Benign' as 0, others as 1)\n",
    "df_test['Label'] = df_test['Label'].map({'Benign': 0}).fillna(1).astype(int)\n",
    "df_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_test[feature_cols] = scaler.transform(imputer.transform(df_test[feature_cols]))\n",
    "df_test[feature_cols] = df_test[feature_cols].astype(np.float64)\n",
    "test_dataset = preprocess_dynamic_by_count(df_test, feature_cols, window_size=50)\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_trues = [], []\n",
    "print(\"\\nüß™ Evaluating A3TGCN2 on 2018 test set (grouped by fixed event count)...\")\n",
    "with torch.no_grad():\n",
    "    for t in range(len(test_dataset.features)):\n",
    "        edge_index = test_dataset.edge_indices[t]\n",
    "        edge_src = edge_index[0]\n",
    "        edge_dst = edge_index[1]\n",
    "        y_hat = model(test_dataset.features[t], edge_index, None, edge_src, edge_dst)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_trues.extend(test_dataset.targets[t].cpu().numpy())\n",
    "\n",
    "print(\"\\n=== Final Test Set Evaluation ===\")\n",
    "print(classification_report(all_trues, all_preds, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_trues, all_preds))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
