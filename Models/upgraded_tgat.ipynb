{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6757d7a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "\u001b[0;32m<ipython-input-23-b2aa86fc0b18>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m    117\u001b[0m \u001b[0;31m# Training Loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    118\u001b[0m \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 119\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFTGNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    121\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
      "\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n",
      "\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n",
      "\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n",
      "\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n",
      "\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n",
      "\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m   1328\u001b[0m                     )\n",
      "\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n",
      "\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import entropy\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric_temporal.signal import DynamicGraphTemporalSignal\n",
    "from torch_geometric_temporal.nn.recurrent import A3TGCN2\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------------\n",
    "# 0. Set device\n",
    "# ---------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load & Preprocess Training Data\n",
    "# ---------------------------\n",
    "df_train = pd.read_csv('/content/drive/MyDrive/dsa4263/2017_data.csv')\n",
    "df_train['Timestamp'] = pd.to_datetime(df_train['Timestamp'], errors='coerce')\n",
    "df_train = df_train.dropna(subset=['Timestamp'])\n",
    "\n",
    "# Map label: benign flows become 0; everything else becomes 1\n",
    "df_train['Label'] = df_train['Label'].map({'BENIGN': 0, 'Benign': 0}).fillna(1).astype(int)\n",
    "\n",
    "# Define feature columns (exclude graph‐related columns)\n",
    "graph_cols = ['Source IP', 'Destination IP', 'Label']\n",
    "feature_cols = [col for col in df_train.columns if col not in graph_cols + ['Flow ID', 'Timestamp']]\n",
    "\n",
    "# Clean numeric features\n",
    "df_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "scaler = StandardScaler()\n",
    "df_train[feature_cols] = scaler.fit_transform(imputer.fit_transform(df_train[feature_cols]))\n",
    "df_train[feature_cols] = df_train[feature_cols].astype(np.float64)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Enriched Node Feature Engineering\n",
    "# ---------------------------\n",
    "def build_node_features(sub_df):\n",
    "    \"\"\"\n",
    "    For a given sub-dataframe (e.g. a window of events),\n",
    "    aggregate the edge features per node and compute:\n",
    "      - Number of times a node appears as source,\n",
    "      - Number of times as destination,\n",
    "      - Entropy over destination IPs for that node.\n",
    "    The final node feature vector has dimension: len(feature_cols) + 3.\n",
    "    \"\"\"\n",
    "    src_counts = sub_df['Source IP'].value_counts().to_dict()\n",
    "    dst_counts = sub_df['Destination IP'].value_counts().to_dict()\n",
    "    ip_dst_entropy = defaultdict(list)\n",
    "    ip_feat_sum = defaultdict(lambda: np.zeros(len(feature_cols)))\n",
    "\n",
    "    # Accumulate feature vectors per source IP and record destination occurrences.\n",
    "    for _, row in sub_df.iterrows():\n",
    "        src, dst = row['Source IP'], row['Destination IP']\n",
    "        ip_feat_sum[src] += row[feature_cols].values.astype(np.float64)\n",
    "        ip_dst_entropy[src].append(dst)\n",
    "\n",
    "    # Get all IPs present (from sources or destinations)\n",
    "    all_ips = set(sub_df['Source IP']).union(set(sub_df['Destination IP']))\n",
    "    ip_to_idx = {ip: i for i, ip in enumerate(all_ips)}\n",
    "\n",
    "    # Build the node features: first the aggregated feature vector,\n",
    "    # then append the counts (as source & destination) and entropy.\n",
    "    node_features = np.zeros((len(ip_to_idx), len(feature_cols) + 3))\n",
    "    for ip, idx in ip_to_idx.items():\n",
    "        feat = ip_feat_sum[ip]\n",
    "        count_src = src_counts.get(ip, 0)\n",
    "        count_dst = dst_counts.get(ip, 0)\n",
    "        ent = entropy(pd.Series(ip_dst_entropy[ip]).value_counts(normalize=True)) if ip_dst_entropy[ip] else 0\n",
    "        node_features[idx] = np.concatenate([feat, [count_src, count_dst, ent]])\n",
    "    return torch.tensor(node_features, dtype=torch.float32), ip_to_idx\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build a Dynamic Graph (Grouping by a Fixed Number of Events)\n",
    "# ---------------------------\n",
    "def preprocess_dynamic_by_count(df, feature_cols, window_size=200):\n",
    "    \"\"\"\n",
    "    Create a dynamic graph signal from the DataFrame without minute binning.\n",
    "    The data is divided into windows of consecutive events (default = 50 events per window).\n",
    "    Each window is used to compute enriched node features, edge indices, edge attributes,\n",
    "    and targets.\n",
    "    \"\"\"\n",
    "    # Sort DataFrame by timestamp to preserve temporal order\n",
    "    df = df.sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "\n",
    "    num_windows = len(df) // window_size\n",
    "    edge_indices = []\n",
    "    edge_attrs = []\n",
    "    targets = []\n",
    "    features = []\n",
    "\n",
    "    for i in range(num_windows):\n",
    "        sub_df = df.iloc[i * window_size: (i + 1) * window_size]\n",
    "        node_feats, ip_to_idx = build_node_features(sub_df)\n",
    "\n",
    "        # Build edge index for the current window from source/destination IPs\n",
    "        try:\n",
    "            edge_index = torch.tensor([\n",
    "                [ip_to_idx[s] for s in sub_df['Source IP']],\n",
    "                [ip_to_idx[d] for d in sub_df['Destination IP']]\n",
    "            ], dtype=torch.long)\n",
    "        except KeyError:\n",
    "            # Skip this window if a mapping issue occurs\n",
    "            continue\n",
    "\n",
    "        # Edge attributes are the original (scaled) features\n",
    "        edge_attr = torch.tensor(sub_df[feature_cols].values.astype(np.float64), dtype=torch.float32)\n",
    "        target = torch.tensor(sub_df['Label'].values, dtype=torch.long)\n",
    "\n",
    "        edge_indices.append(edge_index)\n",
    "        edge_attrs.append(edge_attr)\n",
    "        targets.append(target)\n",
    "        # The node features are expected in shape: (batch_size, num_nodes, in_channels, 1)\n",
    "        features.append(node_feats.unsqueeze(0).unsqueeze(-1))\n",
    "\n",
    "    return DynamicGraphTemporalSignal(edge_indices, edge_attrs, features, targets)\n",
    "\n",
    "# Build the dynamic graph for training using fixed event grouping\n",
    "train_dataset = preprocess_dynamic_by_count(df_train, feature_cols, window_size=50)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Define the A3TGCN2-based Edge Classifier\n",
    "# ---------------------------\n",
    "class A3TGCN2_EdgeClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=2, batch_size=1):\n",
    "        \"\"\"\n",
    "        in_channels should match the dimension of the enriched node features (len(feature_cols) + 3).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.recurrent = A3TGCN2(in_channels=in_channels, out_channels=64, periods=1, batch_size=batch_size)\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight, edge_src, edge_dst):\n",
    "        x = x.to(device)\n",
    "        edge_index = edge_index.to(device)\n",
    "        edge_src = edge_src.to(device)\n",
    "        edge_dst = edge_dst.to(device)\n",
    "        if edge_weight is not None:\n",
    "            edge_weight = edge_weight.to(device)\n",
    "        # Get node representations from the recurrent module. Note we take the first element of the tuple.\n",
    "        h = self.recurrent(x, edge_index, edge_weight)[0]\n",
    "        h_src = h[edge_src]\n",
    "        h_dst = h[edge_dst]\n",
    "        # Concatenate the source and destination node features for each edge\n",
    "        edge_input = torch.cat([h_src, h_dst], dim=1)\n",
    "        return self.edge_mlp(edge_input)\n",
    "\n",
    "# The enriched node features have dimension: len(feature_cols) + 3\n",
    "in_channels = len(feature_cols) + 3\n",
    "model = A3TGCN2_EdgeClassifier(in_channels=in_channels).to(device)\n",
    "\n",
    "# ---------------------------\n",
    "# 4.1 Initialize Optimizer, Scheduler, and Loss Function with Weighted Loss\n",
    "# ---------------------------\n",
    "# Here we use the support counts from your evaluation:\n",
    "#   Support for class 0: 1,343,256\n",
    "#   Support for class 1:   575,394\n",
    "num_class0 = 1343256\n",
    "num_class1 = 575394\n",
    "total_samples = num_class0 + num_class1\n",
    "weight_class0 = total_samples / (2.0 * num_class0)  # Inverse frequency weighting\n",
    "weight_class1 = total_samples / (2.0 * num_class1)\n",
    "weights = torch.tensor([weight_class0, weight_class1], dtype=torch.float).to(device)\n",
    "\n",
    "# Use weighted CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# Feel free to adjust the learning rate; here we use a lower LR based on earlier experimentation.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Training Loop\n",
    "# ---------------------------\n",
    "print(\"⏳ Training A3TGCN2 on 2017 data (grouped by fixed event count)...\")\n",
    "model.train()\n",
    "num_epochs = 50  # Adjust the epoch count as needed\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    # Iterate over each snapshot (each window of events)\n",
    "    for t in range(len(train_dataset.features)):\n",
    "        edge_index = train_dataset.edge_indices[t]\n",
    "        edge_src = edge_index[0]\n",
    "        edge_dst = edge_index[1]\n",
    "        # Forward pass: note that we pass None for edge_weight\n",
    "        y_hat = model(train_dataset.features[t], edge_index, None, edge_src, edge_dst)\n",
    "        loss = loss_fn(y_hat, train_dataset.targets[t].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    scheduler.step()  # decay learning rate every `step_size` epochs\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Preprocess & Evaluate on Test Set (2018)\n",
    "# ---------------------------\n",
    "df_test = pd.read_csv('/content/drive/MyDrive/dsa4263/ddos2018_cleaned.csv').iloc[:, :-2]\n",
    "df_test['Timestamp'] = pd.to_datetime(df_test['Timestamp'], errors='coerce')\n",
    "df_test = df_test.dropna(subset=['Timestamp'])\n",
    "\n",
    "# Map label similarly for test (e.g. 'Benign' as 0, others as 1)\n",
    "df_test['Label'] = df_test['Label'].map({'Benign': 0}).fillna(1).astype(int)\n",
    "df_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_test[feature_cols] = scaler.transform(imputer.transform(df_test[feature_cols]))\n",
    "df_test[feature_cols] = df_test[feature_cols].astype(np.float64)\n",
    "test_dataset = preprocess_dynamic_by_count(df_test, feature_cols, window_size=50)\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "all_preds, all_trues = [], []\n",
    "print(\"\\n🧪 Evaluating A3TGCN2 on 2018 test set (grouped by fixed event count)...\")\n",
    "with torch.no_grad():\n",
    "    for t in range(len(test_dataset.features)):\n",
    "        edge_index = test_dataset.edge_indices[t]\n",
    "        edge_src = edge_index[0]\n",
    "        edge_dst = edge_index[1]\n",
    "        y_hat = model(test_dataset.features[t], edge_index, None, edge_src, edge_dst)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_trues.extend(test_dataset.targets[t].cpu().numpy())\n",
    "\n",
    "print(\"\\n=== Final Test Set Evaluation ===\")\n",
    "print(classification_report(all_trues, all_preds, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_trues, all_preds))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
