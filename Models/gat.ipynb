{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f14f55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1-minute batches in training: 342\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 342/342 [01:10<00:00,  4.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2053.5952\n",
      "\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 342/342 [01:09<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No valid graphs in this epoch with both classes.\n",
      "\n",
      "=== Loading and Evaluating Test Set (2018) ===\n",
      "ðŸ§ª Evaluating on 91 1-minute batches from test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [14:05<00:00,  9.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Test Set Evaluation ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6977    0.9887    0.8181   1343290\n",
      "           1     0.0000    0.0000    0.0000    575394\n",
      "\n",
      "    accuracy                         0.6922   1918684\n",
      "   macro avg     0.3489    0.4943    0.4090   1918684\n",
      "weighted avg     0.4885    0.6922    0.5728   1918684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1328094   15196]\n",
      " [ 575394       0]]\n"
     ]
    }
   ],
   "source": [
    "# =============== LOAD TRAINING DATA ===============\n",
    "df = pd.read_csv('/content/drive/MyDrive/dsa4263/2017_data.csv')\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n",
    "df = df.dropna(subset=['Timestamp'])\n",
    "\n",
    "df['Label'] = df['Label'].map({'BENIGN': 0}).fillna(1).astype(int)\n",
    "df['MinuteBin'] = df['Timestamp'].dt.floor('Min')\n",
    "\n",
    "graph_cols = ['Source IP', 'Destination IP', 'Label']\n",
    "feature_cols = [c for c in df.columns if c not in graph_cols + ['Flow ID', 'Timestamp', 'MinuteBin']]\n",
    "\n",
    "# Clean numeric features\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "scaler = StandardScaler()\n",
    "df[feature_cols] = scaler.fit_transform(imputer.fit_transform(df[feature_cols]))\n",
    "df[feature_cols] = df[feature_cols].astype(np.float64)  # Enforce float64\n",
    "\n",
    "# =============== NODE FEATURE ENGINEERING ===============\n",
    "def build_node_features(sub_df):\n",
    "    ip_stats = defaultdict(lambda: np.zeros(len(feature_cols) + 3))\n",
    "    src_counts = sub_df['Source IP'].value_counts().to_dict()\n",
    "    dst_counts = sub_df['Destination IP'].value_counts().to_dict()\n",
    "    ip_dst_entropy = defaultdict(list)\n",
    "\n",
    "    for _, row in sub_df.iterrows():\n",
    "        src, dst = row['Source IP'], row['Destination IP']\n",
    "        ip_stats[src][:len(feature_cols)] += row[feature_cols].values.astype(np.float64)\n",
    "        ip_dst_entropy[src].append(dst)\n",
    "\n",
    "    all_ips = set(sub_df['Source IP']).union(set(sub_df['Destination IP']))\n",
    "    ip_to_idx = {ip: i for i, ip in enumerate(all_ips)}\n",
    "    node_features = np.zeros((len(ip_to_idx), len(feature_cols) + 3))\n",
    "\n",
    "    for ip, idx in ip_to_idx.items():\n",
    "        feat = ip_stats[ip][:len(feature_cols)]\n",
    "        count_src = src_counts.get(ip, 0)\n",
    "        count_dst = dst_counts.get(ip, 0)\n",
    "        ent = entropy(pd.Series(ip_dst_entropy[ip]).value_counts(normalize=True)) if ip_dst_entropy[ip] else 0\n",
    "        node_features[idx] = np.concatenate([feat, [count_src, count_dst, ent]])\n",
    "\n",
    "    return torch.tensor(node_features, dtype=torch.float32), ip_to_idx\n",
    "\n",
    "# =============== GAT MODEL ===============\n",
    "class EdgeGAT(torch.nn.Module):\n",
    "    def __init__(self, node_feat_dim, edge_feat_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATConv(node_feat_dim, hidden_dim, heads=2, concat=True)\n",
    "        self.gat2 = GATConv(2 * hidden_dim, hidden_dim, heads=1)\n",
    "        self.edge_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2 * hidden_dim + edge_feat_dim, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, edge_src, edge_dst):\n",
    "        x = torch.relu(self.gat1(x, edge_index))\n",
    "        x = self.gat2(x, edge_index)\n",
    "        edge_inputs = torch.cat([x[edge_src], x[edge_dst], edge_attr], dim=1)\n",
    "        return self.edge_mlp(edge_inputs)\n",
    "\n",
    "# =============== TRAIN LOOP ===============\n",
    "model = None\n",
    "optimizer = None\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "num_epochs = 5\n",
    "minutes = df['MinuteBin'].unique()\n",
    "print(f\"Total 1-minute batches in training: {len(minutes)}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}\")\n",
    "    epoch_loss = 0\n",
    "    model_initialized = False\n",
    "\n",
    "    for minute in tqdm(minutes):\n",
    "        sub_df = df[df['MinuteBin'] == minute]\n",
    "        if sub_df['Label'].nunique() < 2:\n",
    "            continue\n",
    "\n",
    "        node_feats, ip_to_idx = build_node_features(sub_df)\n",
    "        edge_index = torch.tensor([\n",
    "            [ip_to_idx[s] for s in sub_df['Source IP']],\n",
    "            [ip_to_idx[d] for d in sub_df['Destination IP']]\n",
    "        ], dtype=torch.long)\n",
    "\n",
    "        edge_src = edge_index[0]\n",
    "        edge_dst = edge_index[1]\n",
    "        edge_attr = torch.tensor(sub_df[feature_cols].values.astype(np.float64), dtype=torch.float32)\n",
    "        y = torch.tensor(sub_df['Label'].values, dtype=torch.long)\n",
    "\n",
    "        if model is None:\n",
    "            model = EdgeGAT(node_feat_dim=node_feats.shape[1], edge_feat_dim=edge_attr.shape[1])\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "            model_initialized = True\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(node_feats, edge_index, edge_attr, edge_src, edge_dst)\n",
    "        loss = loss_fn(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    if not model_initialized:\n",
    "        print(\"âš ï¸ No valid graphs in this epoch with both classes.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# =============== EVALUATE ON TEST SET ===============\n",
    "print(\"\\n=== Loading and Evaluating Test Set (2018) ===\")\n",
    "test_df = pd.read_csv('/content/drive/MyDrive/dsa4263/ddos2018_cleaned.csv').iloc[:, :-2]\n",
    "test_df['Timestamp'] = pd.to_datetime(test_df['Timestamp'], errors='coerce')\n",
    "test_df = test_df.dropna(subset=['Timestamp'])\n",
    "\n",
    "test_df['Label'] = test_df['Label'].map({'Benign': 0}).fillna(1).astype(int)\n",
    "test_df['MinuteBin'] = test_df['Timestamp'].dt.floor('Min')\n",
    "\n",
    "test_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "test_df[feature_cols] = scaler.transform(imputer.transform(test_df[feature_cols]))\n",
    "test_df[feature_cols] = test_df[feature_cols].astype(np.float64)\n",
    "\n",
    "test_minutes = test_df['MinuteBin'].unique()\n",
    "all_preds = []\n",
    "all_trues = []\n",
    "\n",
    "print(f\"ðŸ§ª Evaluating on {len(test_minutes)} 1-minute batches from test set...\")\n",
    "\n",
    "model.eval()\n",
    "for minute in tqdm(test_minutes):\n",
    "    sub_df = test_df[test_df['MinuteBin'] == minute]\n",
    "\n",
    "    node_feats, ip_to_idx = build_node_features(sub_df)\n",
    "    try:\n",
    "        edge_index = torch.tensor([\n",
    "            [ip_to_idx[s] for s in sub_df['Source IP']],\n",
    "            [ip_to_idx[d] for d in sub_df['Destination IP']]\n",
    "        ], dtype=torch.long)\n",
    "    except KeyError:\n",
    "        continue  # unseen IPs cannot be mapped\n",
    "\n",
    "    edge_src = edge_index[0]\n",
    "    edge_dst = edge_index[1]\n",
    "    edge_attr = torch.tensor(sub_df[feature_cols].values.astype(np.float64), dtype=torch.float32)\n",
    "    y_true = torch.tensor(sub_df['Label'].values, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(node_feats, edge_index, edge_attr, edge_src, edge_dst)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "    all_preds.extend(preds.cpu().numpy())\n",
    "    all_trues.extend(y_true.cpu().numpy())\n",
    "\n",
    "# =============== FINAL METRICS ===============\n",
    "print(\"\\n=== Final Test Set Evaluation ===\")\n",
    "print(classification_report(all_trues, all_preds, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_trues, all_preds))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
